{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqBnGI1ltMfn"
   },
   "source": [
    "# **Mountain Car (Discrete case)**\n",
    "\n",
    "References :\n",
    "\n",
    "SARSA:  https://github.com/nicklasbekkevold/SARSA-mountaincar\n",
    "\n",
    "SARSA (lambda) : https://github.com/dariopavllo/mountaincar-sarsa-lambda\n",
    "\n",
    "Semi-Gradient SARSA : https://github.com/MikeS96/rl_openai/blob/master/Mountain%20Car.ipynb\n",
    "http://www.incompleteideas.net/book/RLbook2018.pdf#page=267\n",
    "\n",
    "\n",
    "PPO:    https://github.com/rossettisimone/PPO_MOUNTAINCAR_DISCRETE.\n",
    "\n",
    "Reddit: www.reddit.com/r/reinforcementlearning/comments/axp63j/d_state_of_the_art_deeprl_still_struggles_to/.  \n",
    "https://arxiv.org/pdf/1802.05054\n",
    "\n",
    "\n",
    "Documentation pour la librairie tiles3 : http://incompleteideas.net/tiles/tiles3.html\n",
    "\n",
    "\n",
    "Q-learning : https://www.kaggle.com/code/rezafazel63/mountain-car-is-a-classic-reinforcement-learning\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wl4ESb-y_rZ"
   },
   "source": [
    "# Google Colab configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62IGM72q6dRu",
    "outputId": "b11cc843-5fdf-466e-dbf4-acc44e723321"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     - ----------------------------------- 20.5/721.7 kB 217.9 kB/s eta 0:00:04\n",
      "     -- ---------------------------------- 41.0/721.7 kB 393.8 kB/s eta 0:00:02\n",
      "     ---------------- --------------------- 317.4/721.7 kB 2.2 MB/s eta 0:00:01\n",
      "     -------------------------------- ----- 614.4/721.7 kB 3.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 721.7/721.7 kB 3.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting numpy>=1.18.0 (from gym)\n",
      "  Using cached numpy-2.2.0-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting cloudpickle>=1.2.0 (from gym)\n",
      "  Using cached cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Using cached cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Using cached numpy-2.2.0-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827635 sha256=c5e768eb74ed71f877e837ecb8062175c855bb8c2bf82a7efbab05d7061538dd\n",
      "  Stored in directory: c:\\users\\salma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\1c\\77\\9e\\9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, numpy, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.1.0 gym-0.26.2 gym_notices-0.0.8 numpy-2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[classic_control] in c:\\dev\\m2_iasd\\rl\\project\\mountain_car\\.venv\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\dev\\m2_iasd\\rl\\project\\mountain_car\\.venv\\lib\\site-packages (from gym[classic_control]) (2.2.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\dev\\m2_iasd\\rl\\project\\mountain_car\\.venv\\lib\\site-packages (from gym[classic_control]) (3.1.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\dev\\m2_iasd\\rl\\project\\mountain_car\\.venv\\lib\\site-packages (from gym[classic_control]) (0.0.8)\n",
      "Collecting pygame==2.1.0 (from gym[classic_control])\n",
      "  Downloading pygame-2.1.0.tar.gz (5.8 MB)\n",
      "     ---------------------------------------- 0.0/5.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/5.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/5.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/5.8 MB 573.4 kB/s eta 0:00:10\n",
      "     -- ------------------------------------- 0.4/5.8 MB 2.4 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 1.2/5.8 MB 5.4 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 2.4/5.8 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 3.9/5.8 MB 12.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 5.0/5.8 MB 14.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.8/5.8 MB 16.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.8/5.8 MB 14.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [95 lines of output]\n",
      "      \n",
      "      \n",
      "      WARNING, No \"Setup\" File Exists, Running \"buildconfig/config.py\"\n",
      "      Using WINDOWS configuration...\n",
      "      \n",
      "      Making dir :prebuilt_downloads:\n",
      "      Downloading... https://www.libsdl.org/release/SDL2-devel-2.0.16-VC.zip 13d952c333f3c2ebe9b7bc0075b4ad2f784e7584\n",
      "      Unzipping :prebuilt_downloads\\SDL2-devel-2.0.16-VC.zip:\n",
      "      Downloading... https://www.libsdl.org/projects/SDL_image/release/SDL2_image-devel-2.0.5-VC.zip 137f86474691f4e12e76e07d58d5920c8d844d5b\n",
      "      Unzipping :prebuilt_downloads\\SDL2_image-devel-2.0.5-VC.zip:\n",
      "      Downloading... https://www.libsdl.org/projects/SDL_ttf/release/SDL2_ttf-devel-2.0.15-VC.zip 1436df41ebc47ac36e02ec9bda5699e80ff9bd27\n",
      "      Unzipping :prebuilt_downloads\\SDL2_ttf-devel-2.0.15-VC.zip:\n",
      "      Downloading... https://www.libsdl.org/projects/SDL_mixer/release/SDL2_mixer-devel-2.0.4-VC.zip 9097148f4529cf19f805ccd007618dec280f0ecc\n",
      "      Unzipping :prebuilt_downloads\\SDL2_mixer-devel-2.0.4-VC.zip:\n",
      "      Downloading... https://www.pygame.org/ftp/jpegsr9d.zip ed10aa2b5a0fcfe74f8a6f7611aeb346b06a1f99\n",
      "      Unzipping :prebuilt_downloads\\jpegsr9d.zip:\n",
      "      Downloading... https://pygame.org/ftp/prebuilt-x64-pygame-1.9.2-20150922.zip 3a5af3427b3aa13a0aaf5c4cb08daaed341613ed\n",
      "      Unzipping :prebuilt_downloads\\prebuilt-x64-pygame-1.9.2-20150922.zip:\n",
      "      copying into .\\prebuilt-x64\n",
      "      Path for SDL: prebuilt-x64\\SDL2-2.0.16\n",
      "      ...Library directory for SDL: prebuilt-x64/SDL2-2.0.16/lib/x64\n",
      "      ...Include directory for SDL: prebuilt-x64/SDL2-2.0.16/include\n",
      "      Path for FONT: prebuilt-x64\\SDL2_ttf-2.0.15\n",
      "      ...Library directory for FONT: prebuilt-x64/SDL2_ttf-2.0.15/lib/x64\n",
      "      ...Include directory for FONT: prebuilt-x64/SDL2_ttf-2.0.15/include\n",
      "      Path for IMAGE: prebuilt-x64\\SDL2_image-2.0.5\n",
      "      ...Library directory for IMAGE: prebuilt-x64/SDL2_image-2.0.5/lib/x64\n",
      "      ...Include directory for IMAGE: prebuilt-x64/SDL2_image-2.0.5/include\n",
      "      Path for MIXER: prebuilt-x64\\SDL2_mixer-2.0.4\n",
      "      ...Library directory for MIXER: prebuilt-x64/SDL2_mixer-2.0.4/lib/x64\n",
      "      ...Include directory for MIXER: prebuilt-x64/SDL2_mixer-2.0.4/include\n",
      "      Path for PORTMIDI: prebuilt-x64\n",
      "      ...Library directory for PORTMIDI: prebuilt-x64/lib\n",
      "      ...Include directory for PORTMIDI: prebuilt-x64/include\n",
      "      DLL for SDL2: prebuilt-x64/SDL2-2.0.16/lib/x64/SDL2.dll\n",
      "      DLL for SDL2_ttf: prebuilt-x64/SDL2_ttf-2.0.15/lib/x64/SDL2_ttf.dll\n",
      "      DLL for SDL2_image: prebuilt-x64/SDL2_image-2.0.5/lib/x64/SDL2_image.dll\n",
      "      DLL for SDL2_mixer: prebuilt-x64/SDL2_mixer-2.0.4/lib/x64/SDL2_mixer.dll\n",
      "      DLL for portmidi: prebuilt-x64/lib/portmidi.dll\n",
      "      Path for FREETYPE not found.\n",
      "      ...Found include dir but no library dir in prebuilt-x64.\n",
      "      Path for PNG not found.\n",
      "      ...Found include dir but no library dir in prebuilt-x64.\n",
      "      Path for JPEG not found.\n",
      "      ...Found include dir but no library dir in prebuilt-x64.\n",
      "      DLL for freetype: prebuilt-x64/SDL2_ttf-2.0.15/lib/x64/libfreetype-6.dll\n",
      "      \n",
      "      ---\n",
      "      For help with compilation see:\n",
      "          https://www.pygame.org/wiki/CompileWindows\n",
      "      To contribute to pygame development see:\n",
      "          https://www.pygame.org/contribute.html\n",
      "      ---\n",
      "      \n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\salma\\AppData\\Local\\Temp\\pip-install-p5ufn4hx\\pygame_694e459e4d7f4962a9572a64388c3da2\\buildconfig\\config_win.py\", line 336, in configure\n",
      "          from . import vstools\n",
      "        File \"C:\\Users\\salma\\AppData\\Local\\Temp\\pip-install-p5ufn4hx\\pygame_694e459e4d7f4962a9572a64388c3da2\\buildconfig\\vstools.py\", line 5, in <module>\n",
      "          from distutils.msvccompiler import MSVCCompiler, get_build_architecture\n",
      "      ModuleNotFoundError: No module named 'distutils.msvccompiler'\n",
      "      \n",
      "      During handling of the above exception, another exception occurred:\n",
      "      \n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\DEV\\M2_IASD\\RL\\Project\\mountain_car\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"C:\\DEV\\M2_IASD\\RL\\Project\\mountain_car\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\DEV\\M2_IASD\\RL\\Project\\mountain_car\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "          return hook(config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\salma\\AppData\\Local\\Temp\\pip-build-env-xzi3_lor\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 334, in get_requires_for_build_wheel\n",
      "          return self._get_build_requires(config_settings, requirements=[])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\salma\\AppData\\Local\\Temp\\pip-build-env-xzi3_lor\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 304, in _get_build_requires\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\salma\\AppData\\Local\\Temp\\pip-build-env-xzi3_lor\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 522, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\salma\\AppData\\Local\\Temp\\pip-build-env-xzi3_lor\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 388, in <module>\n",
      "        File \"C:\\Users\\salma\\AppData\\Local\\Temp\\pip-install-p5ufn4hx\\pygame_694e459e4d7f4962a9572a64388c3da2\\buildconfig\\config.py\", line 234, in main\n",
      "          deps = CFG.main(**kwds)\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\salma\\AppData\\Local\\Temp\\pip-install-p5ufn4hx\\pygame_694e459e4d7f4962a9572a64388c3da2\\buildconfig\\config_win.py\", line 511, in main\n",
      "          return setup_prebuilt_sdl2(prebuilt_dir)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\salma\\AppData\\Local\\Temp\\pip-install-p5ufn4hx\\pygame_694e459e4d7f4962a9572a64388c3da2\\buildconfig\\config_win.py\", line 471, in setup_prebuilt_sdl2\n",
      "          DEPS.configure()\n",
      "        File \"C:\\Users\\salma\\AppData\\Local\\Temp\\pip-install-p5ufn4hx\\pygame_694e459e4d7f4962a9572a64388c3da2\\buildconfig\\config_win.py\", line 338, in configure\n",
      "          from buildconfig import vstools\n",
      "        File \"C:\\Users\\salma\\AppData\\Local\\Temp\\pip-install-p5ufn4hx\\pygame_694e459e4d7f4962a9572a64388c3da2\\buildconfig\\vstools.py\", line 5, in <module>\n",
      "          from distutils.msvccompiler import MSVCCompiler, get_build_architecture\n",
      "      ModuleNotFoundError: No module named 'distutils.msvccompiler'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyvirtualdisplay\n",
      "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pyvirtualdisplay\n",
      "Successfully installed pyvirtualdisplay-3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install gym[classic_control]\n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0x99ZxXShKq"
   },
   "outputs": [],
   "source": [
    "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "# !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "# !apt-get update > /dev/null 2>&1\n",
    "# !apt-get install cmake > /dev/null 2>&1\n",
    "# !pip install --upgrade setuptools 2>&1\n",
    "# !pip install ez_setup > /dev/null 2>&1\n",
    "# !apt-get install -y xvfb\n",
    "\n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, \"./video\")\n",
    "    return env\n",
    "\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob(\"video/*.mp4\")\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, \"r+b\").read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(\n",
    "            HTML(\n",
    "                data=\"\"\"<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>\"\"\".format(\n",
    "                    encoded.decode(\"ascii\")\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "\n",
    "# A executer avant l'initialisation de env\n",
    "# display = Display(visible=0, size=(1400, 900))\n",
    "# display.start()\n",
    "# env = wrap_env(gym.make('MountainCar-v0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-CUN_-zF4ZI"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YznFMOFRzE0M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# from gym.wrappers.record_video import RecordVideo\n",
    "# import glob\n",
    "# import io\n",
    "# import base64\n",
    "# from IPython.display import HTML\n",
    "# from pyvirtualdisplay import Display\n",
    "# from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yZDd5xPGfhB"
   },
   "source": [
    "# 1. Functions & Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sNaqXF6GlnZ"
   },
   "source": [
    "## 1.1. Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "R_zyyJHF85pZ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "class IHT:\n",
    "    def __init__(self, sizeval):\n",
    "        self.size = sizeval\n",
    "        self.overfullCount = 0\n",
    "        self.dictionary = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            \"Collision table: size: \"\n",
    "            + str(self.size)\n",
    "            + \" overfullCount: \"\n",
    "            + str(self.overfullCount)\n",
    "            + \" dictionary: \"\n",
    "            + str(len(self.dictionary))\n",
    "        )\n",
    "\n",
    "    def count(self):\n",
    "        return len(self.dictionary)\n",
    "\n",
    "    def fullp(self):\n",
    "        return len(self.dictionary) >= self.size\n",
    "\n",
    "    def getindex(self, obj, readonly=False):\n",
    "        if obj in self.dictionary:\n",
    "            return self.dictionary[obj]\n",
    "        elif readonly:\n",
    "            return None\n",
    "        size = self.size\n",
    "        if len(self.dictionary) >= size:\n",
    "            if self.overfullCount == 0:\n",
    "                print(\"IHT full, starting to allow collisions\")\n",
    "            self.overfullCount += 1\n",
    "            return hash(obj) % self.size\n",
    "        else:\n",
    "            self.dictionary[obj] = len(self.dictionary)\n",
    "            return self.dictionary[obj]\n",
    "\n",
    "\n",
    "def hashcoords(coordinates, m, readonly=False):\n",
    "    if isinstance(m, IHT):\n",
    "        return m.getindex(tuple(coordinates), readonly)\n",
    "    if isinstance(m, int):\n",
    "        return hash(tuple(coordinates)) % m\n",
    "    if m is None:\n",
    "        return coordinates\n",
    "\n",
    "\n",
    "def get_tiles(ihtORsize, numtilings, floats, ints=[], readonly=False):\n",
    "    qfloats = [int(math.floor(f * numtilings)) for f in floats]\n",
    "    tiles = []\n",
    "    for tiling in range(numtilings):\n",
    "        tilingX2 = tiling * 2\n",
    "        coords = [tiling]\n",
    "        b = tiling\n",
    "        for q in qfloats:\n",
    "            coords.append((q + b) // numtilings)\n",
    "            b += tilingX2\n",
    "        coords.extend(ints)\n",
    "        tiles.append(hashcoords(coords, ihtORsize, readonly))\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5OJBYeuGoAh",
    "outputId": "8a1bd1aa-0256-4f23-ef35-a168fbed4ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiles activées : [0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "POSITION_MIN = -1.2\n",
    "POSITION_MAX = 0.5\n",
    "VELOCITY_MIN = -0.07\n",
    "VELOCITY_MAX = 0.07\n",
    "\n",
    "iht_size = 2024\n",
    "num_tilings = 8\n",
    "num_tiles = 8\n",
    "\n",
    "iht = IHT(2024)\n",
    "\n",
    "\n",
    "def mc_tile_encoding(state):\n",
    "    \"\"\"\n",
    "    Tile encoding function for states in mountain car gymnasium problem\n",
    "\n",
    "    Args:\n",
    "        - state (tuple): (position, velocity)\n",
    "\n",
    "    Returns:\n",
    "        - The list of the tiles corresponding to the given state\n",
    "    \"\"\"\n",
    "    # Extract the position and velocity from the gymnasium state\n",
    "    position, velocity = state\n",
    "\n",
    "    # Scale position and velocity by multiplying the inputs of each by their scale\n",
    "    position_scale = num_tiles / (POSITION_MAX - POSITION_MIN)\n",
    "    velocity_scale = num_tiles / (VELOCITY_MAX - VELOCITY_MIN)\n",
    "\n",
    "    # Obtain active tiles for current position and velocity\n",
    "    tiles = get_tiles(\n",
    "        iht, num_tilings, [position * position_scale, velocity * velocity_scale]\n",
    "    )\n",
    "\n",
    "    return np.array(tiles)\n",
    "\n",
    "\n",
    "print(\"Tiles activées :\", mc_tile_encoding((-1.0, 0.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WDQitRjLG1N"
   },
   "source": [
    "## 1.2. Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGCiJnKY-CN3"
   },
   "source": [
    "### 1.2.1. SARSA Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        encode_fct,\n",
    "        nb_actions=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        - env_name (gymnasium.Env): The environment to train on.\n",
    "        - nb_actions (int): Number of possible actions.\n",
    "        - encode_fct (callable): Function to encode the state into features (e.g., tile coding).\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.nb_actions = env.action_space.n if nb_actions == None else nb_actions\n",
    "        self.encode_fct = encode_fct\n",
    "        self.q = {}\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\"Initialize q values\"\"\"\n",
    "        self.q = {}\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"Compute Q-value for a state-action pair by summing over the tiles.\"\"\"\n",
    "        return sum(self.q.get((tile, action), 0) for tile in self.encode_fct(state))\n",
    "\n",
    "    def choose_action(self, state, epsilon=None, soft_policy=True):\n",
    "        \"\"\"\n",
    "        Choose an action using an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "        - q (dict): The Q-value dictionary.\n",
    "        - state (hashable): The current state.\n",
    "        - epsilon (float, optional): The probability of exploration (for soft policy).\n",
    "        - soft_policy (bool, optional): If True, use epsilon-greedy; otherwise, choose the action with the highest Q-value.\n",
    "\n",
    "        Returns:\n",
    "        - int: The selected action.\n",
    "\n",
    "        Raises:\n",
    "        - ValueError: If soft_policy is True and epsilon is None.\n",
    "        \"\"\"\n",
    "        if (soft_policy) and (epsilon is None):\n",
    "            raise ValueError(\"Epsilon must be specified when soft_policy is True.\")\n",
    "\n",
    "        if (soft_policy) and (np.random.rand() < epsilon):\n",
    "            return np.random.choice(self.nb_actions)  # Exploration\n",
    "        return np.argmax(\n",
    "            [self.get_q_value(state, action) for action in range(self.nb_actions)]\n",
    "        )\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        nb_episodes,\n",
    "        alpha=0.1,\n",
    "        gamma=0.99,\n",
    "        epsilon=0.1,\n",
    "        use_glei=False,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        SARSA algorithm for on-policy reinforcement learning.\n",
    "\n",
    "        Args:\n",
    "            - nb_episodes (int): Number of episodes to train for.\n",
    "            - alpha (float): Learning rate for updating Q-values.\n",
    "            - gamma (float): Discount factor for future rewards.\n",
    "            - epsilon (float): Initial exploration rate for epsilon-greedy policy.\n",
    "            - use_glei (bool): Whether to use a decaying epsilon (GLEI policy).\n",
    "            - decay_rate (float): Decay rate for epsilon in GLEI policy.\n",
    "            - min_epsilon (float): Minimum epsilon value in GLEI policy.\n",
    "            - verbose (boolean): Print or not informations about training\n",
    "        Returns:\n",
    "            - rewards_historic (list): History of rewards across episodes.\n",
    "        \"\"\"\n",
    "        rewads_historic = []\n",
    "        self.init()\n",
    "\n",
    "        for episode in range(nb_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            action = self.choose_action(state, epsilon=epsilon)\n",
    "\n",
    "            task_completed, episode_over = False, False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not (task_completed or episode_over):\n",
    "                next_state, reward, task_completed, episode_over, _ = self.env.step(\n",
    "                    action\n",
    "                )\n",
    "                next_action = self.choose_action(next_state, epsilon=epsilon)\n",
    "\n",
    "                # Compute the SARSA update\n",
    "                q_current = self.get_q_value(state, action)\n",
    "                q_next = (\n",
    "                    self.get_q_value(next_state, next_action)\n",
    "                    if not (task_completed or episode_over)\n",
    "                    else 0\n",
    "                )\n",
    "                target = reward + gamma * q_next\n",
    "                error = target - q_current\n",
    "\n",
    "                # Update Q-values for all tiles\n",
    "                for tile in self.encode_fct(state):\n",
    "                    self.q[(tile, action)] = (\n",
    "                        self.q.get((tile, action), 0) + alpha * error\n",
    "                    )\n",
    "\n",
    "                # Move to the next state and action\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                total_reward += reward\n",
    "\n",
    "            rewads_historic.append(total_reward)\n",
    "            if verbose:\n",
    "                print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "        return rewads_historic\n",
    "\n",
    "    #         Train the agent multiple times in order to evaluate its training parameters with mean and standard deviation over episodes\n",
    "\n",
    "    def evaluate(\n",
    "        self, num_episodes=10, soft_policy=False, render_mode=None, verbose=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Evaluate the current q values of the agent and print the average reward\n",
    "\n",
    "        Args:\n",
    "            -\n",
    "\n",
    "        Returns:\n",
    "            - avg_reward (float): the average reward on the given evaluated period\n",
    "        \"\"\"\n",
    "        if verbose\n",
    "        env = gym.make(\"MountainCar-v0\", render_mode=render_mode)\n",
    "        total_rewards = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = env.reset()\n",
    "            time_over = False\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not (done or time_over):\n",
    "\n",
    "                # Obtenir les tiles pour l'état actuel\n",
    "                state_tiles = mc_tile_encoding(state)\n",
    "\n",
    "                # Had policy by default for evaluation\n",
    "                action = self.choose_action(state, soft_policy=soft_policy)\n",
    "\n",
    "                # Exécuter l'action choisie\n",
    "                state, reward, done, time_over, _ = env.step(action)\n",
    "                # position, velocity = next_state\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "            if verbose:\n",
    "                print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "        env.close()\n",
    "        print(\n",
    "            f\"Average Total Reward over {num_episodes} episodes: {np.mean(total_rewards)}\"\n",
    "        )\n",
    "\n",
    "    # def plot_moving_averages(rewards_dict, nb_episodes, moving_avg_size):\n",
    "    # \"\"\"\n",
    "    # Plots the moving averages of rewards for multiple reward histories on the same graph,\n",
    "    # with named histories provided in a dictionary.\n",
    "\n",
    "    # Args:\n",
    "    #     - rewards_dict (dict): A dictionary where keys are names of the reward histories\n",
    "    #       and values are the corresponding lists of rewards.\n",
    "    #     - moving_avg_size (int, optional): The size of the moving average window. Default is 20.\n",
    "\n",
    "    # Returns:\n",
    "    #     None\n",
    "    # \"\"\"\n",
    "    # plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # # Loop through each rewards history in the dictionary and compute the moving average\n",
    "    # for name, rewards in rewards_dict.items():\n",
    "\n",
    "    #     # Calculate the moving average by sliding the kernel k = [1/moving_avg_size] * moving_avg_size\n",
    "    #     moving_avg_rewards = np.convolve(rewards, np.ones(moving_avg_size) / moving_avg_size, mode='valid')\n",
    "\n",
    "    #     plt.plot(\n",
    "    #         range(len(moving_avg_rewards)),\n",
    "    #         moving_avg_rewards,\n",
    "    #         label=name\n",
    "    #     )\n",
    "\n",
    "    # # Add labels, title, legend, and grid\n",
    "    # plt.xlabel(\"Number of Episodes - Moving Average Window Size\")\n",
    "    # plt.ylabel(\"Moving Average of Rewards\")\n",
    "    # plt.title(f\"Comparison of {moving_avg_size}-Episode Moving Averages Across Reward Histories (trained on {nb_episodes} episodes)\")\n",
    "    # plt.legend()\n",
    "    # plt.grid()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. Experimentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -106.0\n",
      "Episode 2: Total Reward = -89.0\n",
      "Episode 3: Total Reward = -105.0\n",
      "Episode 4: Total Reward = -153.0\n",
      "Episode 5: Total Reward = -147.0\n",
      "Episode 6: Total Reward = -106.0\n",
      "Episode 7: Total Reward = -141.0\n",
      "Episode 8: Total Reward = -105.0\n",
      "Episode 9: Total Reward = -139.0\n",
      "Episode 10: Total Reward = -147.0\n",
      "Average Total Reward over 10 episodes: -123.8\n"
     ]
    }
   ],
   "source": [
    "v0_agent = SarsaAgent(env=gym.make(\"MountainCar-v0\"), encode_fct=mc_tile_encoding)\n",
    "\n",
    "rewards_historic = v0_agent.train(\n",
    "    nb_episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.001\n",
    ")\n",
    "v0_agent.evaluate(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UclLyzGSAmjK",
    "outputId": "cf6d82d4-480d-47d1-c291-9b47bbe233ac"
   },
   "outputs": [],
   "source": [
    "def test_policy(Q, num_episodes=10, render=False):\n",
    "    render = \"human\" if render else None\n",
    "    env = gym.make(\"MountainCar-v0\", render_mode=render)\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        time_over = False\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not (done or time_over):\n",
    "\n",
    "            # Obtenir les tiles pour l'état actuel\n",
    "            state_tiles = mc_tile_encoding(state)\n",
    "\n",
    "            # Choisir l'action qui maximise Q(s, a)\n",
    "            q_values = [\n",
    "                sum(Q.get((tile, action), 0) for tile in state_tiles)\n",
    "                for action in range(env.action_space.n)\n",
    "            ]\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "            # Exécuter l'action choisie\n",
    "            state, reward, done, time_over, _ = env.step(action)\n",
    "            # position, velocity = next_state\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    print(\n",
    "        f\"Average Total Reward over {num_episodes} episodes: {np.mean(total_rewards)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -105.0\n",
      "Episode 2: Total Reward = -105.0\n",
      "Episode 3: Total Reward = -92.0\n",
      "Episode 4: Total Reward = -106.0\n",
      "Episode 5: Total Reward = -105.0\n",
      "Episode 6: Total Reward = -178.0\n",
      "Episode 7: Total Reward = -96.0\n",
      "Episode 8: Total Reward = -93.0\n",
      "Episode 9: Total Reward = -105.0\n",
      "Episode 10: Total Reward = -105.0\n",
      "Average Total Reward over 10 episodes: -109.0\n"
     ]
    }
   ],
   "source": [
    "# Tester la politique optimale\n",
    "test_policy(Q, num_episodes=10, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -107.0\n",
      "Episode 2: Total Reward = -107.0\n",
      "Episode 3: Total Reward = -95.0\n",
      "Episode 4: Total Reward = -159.0\n",
      "Episode 5: Total Reward = -107.0\n",
      "Episode 6: Total Reward = -96.0\n",
      "Episode 7: Total Reward = -90.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 16:54:34.489 Python[13207:3704629] Warning: Window move completed without beginning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8: Total Reward = -178.0\n",
      "Episode 9: Total Reward = -108.0\n",
      "Episode 10: Total Reward = -87.0\n",
      "Average Total Reward over 10 episodes: -113.4\n"
     ]
    }
   ],
   "source": [
    "# Tester la politique optimale\n",
    "test_policy(v0_agent.q, num_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"MountainCar-v0\",\n",
    "    render_mode=\"human\",  # Cas discret\n",
    ")\n",
    "\n",
    "env.reset()\n",
    "print(f\"State space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(array([-0.49266914,  0.00077414], dtype=float32), -1.0, False, False, {})\n",
      "-1.0\n",
      "1\n",
      "(array([-0.49112663,  0.0015425 ], dtype=float32), -1.0, False, False, {})\n",
      "-2.0\n",
      "2\n",
      "(array([-0.48882726,  0.00229935], dtype=float32), -1.0, False, False, {})\n",
      "-3.0\n",
      "3\n",
      "(array([-0.48578823,  0.00303903], dtype=float32), -1.0, False, False, {})\n",
      "-4.0\n",
      "4\n",
      "(array([-0.48203218,  0.00375606], dtype=float32), -1.0, False, False, {})\n",
      "-5.0\n",
      "5\n",
      "(array([-0.47758707,  0.00444512], dtype=float32), -1.0, False, False, {})\n",
      "-6.0\n",
      "6\n",
      "(array([-0.47248593,  0.00510113], dtype=float32), -1.0, False, False, {})\n",
      "-7.0\n",
      "7\n",
      "(array([-0.46676666,  0.00571928], dtype=float32), -1.0, False, False, {})\n",
      "-8.0\n",
      "8\n",
      "(array([-0.46047154,  0.0062951 ], dtype=float32), -1.0, False, False, {})\n",
      "-9.0\n",
      "9\n",
      "(array([-0.45364708,  0.00682447], dtype=float32), -1.0, False, False, {})\n",
      "-10.0\n",
      "10\n",
      "(array([-0.4463434 ,  0.00730368], dtype=float32), -1.0, False, False, {})\n",
      "-11.0\n",
      "11\n",
      "(array([-0.43861398,  0.00772943], dtype=float32), -1.0, False, False, {})\n",
      "-12.0\n",
      "12\n",
      "(array([-0.43051502,  0.00809893], dtype=float32), -1.0, False, False, {})\n",
      "-13.0\n",
      "13\n",
      "(array([-0.4221052 ,  0.00840984], dtype=float32), -1.0, False, False, {})\n",
      "-14.0\n",
      "14\n",
      "(array([-0.41344485,  0.00866035], dtype=float32), -1.0, False, False, {})\n",
      "-15.0\n",
      "15\n",
      "(array([-0.4045957 ,  0.00884915], dtype=float32), -1.0, False, False, {})\n",
      "-16.0\n",
      "16\n",
      "(array([-0.39562023,  0.00897546], dtype=float32), -1.0, False, False, {})\n",
      "-17.0\n",
      "17\n",
      "(array([-0.3865812 ,  0.00903903], dtype=float32), -1.0, False, False, {})\n",
      "-18.0\n",
      "18\n",
      "(array([-0.3775411,  0.0090401], dtype=float32), -1.0, False, False, {})\n",
      "-19.0\n",
      "19\n",
      "(array([-0.3685617 ,  0.00897938], dtype=float32), -1.0, False, False, {})\n",
      "-20.0\n",
      "20\n",
      "(array([-0.35970366,  0.00885808], dtype=float32), -1.0, False, False, {})\n",
      "-21.0\n",
      "21\n",
      "(array([-0.35102585,  0.0086778 ], dtype=float32), -1.0, False, False, {})\n",
      "-22.0\n",
      "22\n",
      "(array([-0.3425853 ,  0.00844055], dtype=float32), -1.0, False, False, {})\n",
      "-23.0\n",
      "23\n",
      "(array([-0.33443663,  0.00814869], dtype=float32), -1.0, False, False, {})\n",
      "-24.0\n",
      "24\n",
      "(array([-0.3266317 ,  0.00780491], dtype=float32), -1.0, False, False, {})\n",
      "-25.0\n",
      "25\n",
      "(array([-0.31921956,  0.00741213], dtype=float32), -1.0, False, False, {})\n",
      "-26.0\n",
      "26\n",
      "(array([-0.31224602,  0.00697354], dtype=float32), -1.0, False, False, {})\n",
      "-27.0\n",
      "27\n",
      "(array([-0.30575353,  0.0064925 ], dtype=float32), -1.0, False, False, {})\n",
      "-28.0\n",
      "28\n",
      "(array([-0.29978102,  0.0059725 ], dtype=float32), -1.0, False, False, {})\n",
      "-29.0\n",
      "29\n",
      "(array([-0.29436383,  0.00541719], dtype=float32), -1.0, False, False, {})\n",
      "-30.0\n",
      "30\n",
      "(array([-0.28953356,  0.00483028], dtype=float32), -1.0, False, False, {})\n",
      "-31.0\n",
      "31\n",
      "(array([-0.28531802,  0.00421554], dtype=float32), -1.0, False, False, {})\n",
      "-32.0\n",
      "32\n",
      "(array([-0.28174123,  0.00357679], dtype=float32), -1.0, False, False, {})\n",
      "-33.0\n",
      "33\n",
      "(array([-0.27882335,  0.00291788], dtype=float32), -1.0, False, False, {})\n",
      "-34.0\n",
      "34\n",
      "(array([-0.2765807 ,  0.00224267], dtype=float32), -1.0, False, False, {})\n",
      "-35.0\n",
      "35\n",
      "(array([-0.27502567,  0.001555  ], dtype=float32), -1.0, False, False, {})\n",
      "-36.0\n",
      "36\n",
      "(array([-0.27416694,  0.00085875], dtype=float32), -1.0, False, False, {})\n",
      "-37.0\n",
      "37\n",
      "(array([-2.7400917e-01,  1.5777275e-04], dtype=float32), -1.0, False, False, {})\n",
      "-38.0\n",
      "38\n",
      "(array([-0.27455324, -0.00054407], dtype=float32), -1.0, False, False, {})\n",
      "-39.0\n",
      "39\n",
      "(array([-0.27579615, -0.00124292], dtype=float32), -1.0, False, False, {})\n",
      "-40.0\n",
      "40\n",
      "(array([-0.27773106, -0.00193492], dtype=float32), -1.0, False, False, {})\n",
      "-41.0\n",
      "41\n",
      "(array([-0.2803473 , -0.00261621], dtype=float32), -1.0, False, False, {})\n",
      "-42.0\n",
      "42\n",
      "(array([-0.28363022, -0.00328293], dtype=float32), -1.0, False, False, {})\n",
      "-43.0\n",
      "43\n",
      "(array([-0.28756145, -0.00393122], dtype=float32), -1.0, False, False, {})\n",
      "-44.0\n",
      "44\n",
      "(array([-0.29211864, -0.00455722], dtype=float32), -1.0, False, False, {})\n",
      "-45.0\n",
      "45\n",
      "(array([-0.29727575, -0.00515711], dtype=float32), -1.0, False, False, {})\n",
      "-46.0\n",
      "46\n",
      "(array([-0.30300283, -0.00572708], dtype=float32), -1.0, False, False, {})\n",
      "-47.0\n",
      "47\n",
      "(array([-0.30926624, -0.0062634 ], dtype=float32), -1.0, False, False, {})\n",
      "-48.0\n",
      "48\n",
      "(array([-0.31602865, -0.0067624 ], dtype=float32), -1.0, False, False, {})\n",
      "-49.0\n",
      "49\n",
      "(array([-0.32324913, -0.00722049], dtype=float32), -1.0, False, False, {})\n",
      "-50.0\n",
      "50\n",
      "(array([-0.3308834 , -0.00763426], dtype=float32), -1.0, False, False, {})\n",
      "-51.0\n",
      "51\n",
      "(array([-0.33888385, -0.00800044], dtype=float32), -1.0, False, False, {})\n",
      "-52.0\n",
      "52\n",
      "(array([-0.34719983, -0.00831599], dtype=float32), -1.0, False, False, {})\n",
      "-53.0\n",
      "53\n",
      "(array([-0.35577792, -0.00857809], dtype=float32), -1.0, False, False, {})\n",
      "-54.0\n",
      "54\n",
      "(array([-0.36456215, -0.00878424], dtype=float32), -1.0, False, False, {})\n",
      "-55.0\n",
      "55\n",
      "(array([-0.37349445, -0.00893227], dtype=float32), -1.0, False, False, {})\n",
      "-56.0\n",
      "56\n",
      "(array([-0.38251483, -0.00902039], dtype=float32), -1.0, False, False, {})\n",
      "-57.0\n",
      "57\n",
      "(array([-0.39156204, -0.00904721], dtype=float32), -1.0, False, False, {})\n",
      "-58.0\n",
      "58\n",
      "(array([-0.40057382, -0.00901179], dtype=float32), -1.0, False, False, {})\n",
      "-59.0\n",
      "59\n",
      "(array([-0.40948752, -0.00891367], dtype=float32), -1.0, False, False, {})\n",
      "-60.0\n",
      "60\n",
      "(array([-0.4182404 , -0.00875289], dtype=float32), -1.0, False, False, {})\n",
      "-61.0\n",
      "61\n",
      "(array([-0.4267704 , -0.00852999], dtype=float32), -1.0, False, False, {})\n",
      "-62.0\n",
      "62\n",
      "(array([-0.43501642, -0.00824603], dtype=float32), -1.0, False, False, {})\n",
      "-63.0\n",
      "63\n",
      "(array([-0.44291902, -0.00790261], dtype=float32), -1.0, False, False, {})\n",
      "-64.0\n",
      "64\n",
      "(array([-0.45042083, -0.00750181], dtype=float32), -1.0, False, False, {})\n",
      "-65.0\n",
      "65\n",
      "(array([-0.45746708, -0.00704625], dtype=float32), -1.0, False, False, {})\n",
      "-66.0\n",
      "66\n",
      "(array([-0.46400607, -0.00653899], dtype=float32), -1.0, False, False, {})\n",
      "-67.0\n",
      "67\n",
      "(array([-0.46998963, -0.00598356], dtype=float32), -1.0, False, False, {})\n",
      "-68.0\n",
      "68\n",
      "(array([-0.4753735, -0.0053839], dtype=float32), -1.0, False, False, {})\n",
      "-69.0\n",
      "69\n",
      "(array([-0.48011786, -0.00474432], dtype=float32), -1.0, False, False, {})\n",
      "-70.0\n",
      "70\n",
      "(array([-0.48418736, -0.00406951], dtype=float32), -1.0, False, False, {})\n",
      "-71.0\n",
      "71\n",
      "(array([-0.48755175, -0.0033644 ], dtype=float32), -1.0, False, False, {})\n",
      "-72.0\n",
      "72\n",
      "(array([-0.49018598, -0.00263423], dtype=float32), -1.0, False, False, {})\n",
      "-73.0\n",
      "73\n",
      "(array([-0.4920704 , -0.00188441], dtype=float32), -1.0, False, False, {})\n",
      "-74.0\n",
      "74\n",
      "(array([-0.4931909 , -0.00112052], dtype=float32), -1.0, False, False, {})\n",
      "-75.0\n",
      "75\n",
      "(array([-4.9353918e-01, -3.4826060e-04], dtype=float32), -1.0, False, False, {})\n",
      "-76.0\n",
      "76\n",
      "(array([-4.9311256e-01,  4.2659784e-04], dtype=float32), -1.0, False, False, {})\n",
      "-77.0\n",
      "77\n",
      "(array([-0.4919143 ,  0.00119827], dtype=float32), -1.0, False, False, {})\n",
      "-78.0\n",
      "78\n",
      "(array([-0.4899533 ,  0.00196099], dtype=float32), -1.0, False, False, {})\n",
      "-79.0\n",
      "79\n",
      "(array([-0.48724422,  0.00270908], dtype=float32), -1.0, False, False, {})\n",
      "-80.0\n",
      "80\n",
      "(array([-0.48380727,  0.00343696], dtype=float32), -1.0, False, False, {})\n",
      "-81.0\n",
      "81\n",
      "(array([-0.47966802,  0.00413923], dtype=float32), -1.0, False, False, {})\n",
      "-82.0\n",
      "82\n",
      "(array([-0.47485733,  0.00481071], dtype=float32), -1.0, False, False, {})\n",
      "-83.0\n",
      "83\n",
      "(array([-0.46941087,  0.00544645], dtype=float32), -1.0, False, False, {})\n",
      "-84.0\n",
      "84\n",
      "(array([-0.46336904,  0.00604183], dtype=float32), -1.0, False, False, {})\n",
      "-85.0\n",
      "85\n",
      "(array([-0.4567765 ,  0.00659256], dtype=float32), -1.0, False, False, {})\n",
      "-86.0\n",
      "86\n",
      "(array([-0.44968176,  0.00709474], dtype=float32), -1.0, False, False, {})\n",
      "-87.0\n",
      "87\n",
      "(array([-0.44213685,  0.00754489], dtype=float32), -1.0, False, False, {})\n",
      "-88.0\n",
      "88\n",
      "(array([-0.43419686,  0.00793999], dtype=float32), -1.0, False, False, {})\n",
      "-89.0\n",
      "89\n",
      "(array([-0.42591938,  0.00827749], dtype=float32), -1.0, False, False, {})\n",
      "-90.0\n",
      "90\n",
      "(array([-0.41736406,  0.00855533], dtype=float32), -1.0, False, False, {})\n",
      "-91.0\n",
      "91\n",
      "(array([-0.40859205,  0.00877199], dtype=float32), -1.0, False, False, {})\n",
      "-92.0\n",
      "92\n",
      "(array([-0.3996656 ,  0.00892645], dtype=float32), -1.0, False, False, {})\n",
      "-93.0\n",
      "93\n",
      "(array([-0.39064738,  0.00901822], dtype=float32), -1.0, False, False, {})\n",
      "-94.0\n",
      "94\n",
      "(array([-0.38160005,  0.00904732], dtype=float32), -1.0, False, False, {})\n",
      "-95.0\n",
      "95\n",
      "(array([-0.3725858 ,  0.00901425], dtype=float32), -1.0, False, False, {})\n",
      "-96.0\n",
      "96\n",
      "(array([-0.36366582,  0.00892   ], dtype=float32), -1.0, False, False, {})\n",
      "-97.0\n",
      "97\n",
      "(array([-0.35489982,  0.00876599], dtype=float32), -1.0, False, False, {})\n",
      "-98.0\n",
      "98\n",
      "(array([-0.34634575,  0.00855408], dtype=float32), -1.0, False, False, {})\n",
      "-99.0\n",
      "99\n",
      "(array([-0.3380593 ,  0.00828645], dtype=float32), -1.0, False, False, {})\n",
      "-100.0\n",
      "100\n",
      "(array([-0.33009365,  0.00796566], dtype=float32), -1.0, False, False, {})\n",
      "-101.0\n",
      "101\n",
      "(array([-0.32249913,  0.00759452], dtype=float32), -1.0, False, False, {})\n",
      "-102.0\n",
      "102\n",
      "(array([-0.31532302,  0.00717611], dtype=float32), -1.0, False, False, {})\n",
      "-103.0\n",
      "103\n",
      "(array([-0.3086093 ,  0.00671372], dtype=float32), -1.0, False, False, {})\n",
      "-104.0\n",
      "104\n",
      "(array([-0.3023985 ,  0.00621079], dtype=float32), -1.0, False, False, {})\n",
      "-105.0\n",
      "105\n",
      "(array([-0.29672763,  0.00567089], dtype=float32), -1.0, False, False, {})\n",
      "-106.0\n",
      "106\n",
      "(array([-0.2916299 ,  0.00509772], dtype=float32), -1.0, False, False, {})\n",
      "-107.0\n",
      "107\n",
      "(array([-0.2871349 ,  0.00449501], dtype=float32), -1.0, False, False, {})\n",
      "-108.0\n",
      "108\n",
      "(array([-0.2832683 ,  0.00386658], dtype=float32), -1.0, False, False, {})\n",
      "-109.0\n",
      "109\n",
      "(array([-0.28005204,  0.00321626], dtype=float32), -1.0, False, False, {})\n",
      "-110.0\n",
      "110\n",
      "(array([-0.27750415,  0.00254789], dtype=float32), -1.0, False, False, {})\n",
      "-111.0\n",
      "111\n",
      "(array([-0.27563882,  0.00186534], dtype=float32), -1.0, False, False, {})\n",
      "-112.0\n",
      "112\n",
      "(array([-0.27446634,  0.00117247], dtype=float32), -1.0, False, False, {})\n",
      "-113.0\n",
      "113\n",
      "(array([-0.2739932 ,  0.00047314], dtype=float32), -1.0, False, False, {})\n",
      "-114.0\n",
      "114\n",
      "(array([-2.7422199e-01, -2.2878814e-04], dtype=float32), -1.0, False, False, {})\n",
      "-115.0\n",
      "115\n",
      "(array([-0.27515143, -0.00092946], dtype=float32), -1.0, False, False, {})\n",
      "-116.0\n",
      "116\n",
      "(array([-0.27677646, -0.00162502], dtype=float32), -1.0, False, False, {})\n",
      "-117.0\n",
      "117\n",
      "(array([-0.27908805, -0.0023116 ], dtype=float32), -1.0, False, False, {})\n",
      "-118.0\n",
      "118\n",
      "(array([-0.2820734 , -0.00298535], dtype=float32), -1.0, False, False, {})\n",
      "-119.0\n",
      "119\n",
      "(array([-0.2857158 , -0.00364239], dtype=float32), -1.0, False, False, {})\n",
      "-120.0\n",
      "120\n",
      "(array([-0.2899947 , -0.00427888], dtype=float32), -1.0, False, False, {})\n",
      "-121.0\n",
      "121\n",
      "(array([-0.29488567, -0.00489098], dtype=float32), -1.0, False, False, {})\n",
      "-122.0\n",
      "122\n",
      "(array([-0.30036053, -0.00547487], dtype=float32), -1.0, False, False, {})\n",
      "-123.0\n",
      "123\n",
      "(array([-0.3063873 , -0.00602677], dtype=float32), -1.0, False, False, {})\n",
      "-124.0\n",
      "124\n",
      "(array([-0.3129303 , -0.00654299], dtype=float32), -1.0, False, False, {})\n",
      "-125.0\n",
      "125\n",
      "(array([-0.3199502, -0.0070199], dtype=float32), -1.0, False, False, {})\n",
      "-126.0\n",
      "126\n",
      "(array([-0.3274042 , -0.00745401], dtype=float32), -1.0, False, False, {})\n",
      "-127.0\n",
      "127\n",
      "(array([-0.33524615, -0.00784196], dtype=float32), -1.0, False, False, {})\n",
      "-128.0\n",
      "128\n",
      "(array([-0.3434268 , -0.00818063], dtype=float32), -1.0, False, False, {})\n",
      "-129.0\n",
      "129\n",
      "(array([-0.35189384, -0.00846707], dtype=float32), -1.0, False, False, {})\n",
      "-130.0\n",
      "130\n",
      "(array([-0.3605925 , -0.00869866], dtype=float32), -1.0, False, False, {})\n",
      "-131.0\n",
      "131\n",
      "(array([-0.3694656 , -0.00887306], dtype=float32), -1.0, False, False, {})\n",
      "-132.0\n",
      "132\n",
      "(array([-0.37845388, -0.0089883 ], dtype=float32), -1.0, False, False, {})\n",
      "-133.0\n",
      "133\n",
      "(array([-0.38749668, -0.00904281], dtype=float32), -1.0, False, False, {})\n",
      "-134.0\n",
      "134\n",
      "(array([-0.39653215, -0.00903545], dtype=float32), -1.0, False, False, {})\n",
      "-135.0\n",
      "135\n",
      "(array([-0.40549767, -0.00896554], dtype=float32), -1.0, False, False, {})\n",
      "-136.0\n",
      "136\n",
      "(array([-0.41433057, -0.00883288], dtype=float32), -1.0, False, False, {})\n",
      "-137.0\n",
      "137\n",
      "(array([-0.42296836, -0.00863779], dtype=float32), -1.0, False, False, {})\n",
      "-138.0\n",
      "138\n",
      "(array([-0.43134946, -0.00838111], dtype=float32), -1.0, False, False, {})\n",
      "-139.0\n",
      "139\n",
      "(array([-0.43941364, -0.00806418], dtype=float32), -1.0, False, False, {})\n",
      "-140.0\n",
      "140\n",
      "(array([-0.44710252, -0.00768888], dtype=float32), -1.0, False, False, {})\n",
      "-141.0\n",
      "141\n",
      "(array([-0.4543601 , -0.00725758], dtype=float32), -1.0, False, False, {})\n",
      "-142.0\n",
      "142\n",
      "(array([-0.46113324, -0.00677314], dtype=float32), -1.0, False, False, {})\n",
      "-143.0\n",
      "143\n",
      "(array([-0.46737215, -0.0062389 ], dtype=float32), -1.0, False, False, {})\n",
      "-144.0\n",
      "144\n",
      "(array([-0.47303075, -0.0056586 ], dtype=float32), -1.0, False, False, {})\n",
      "-145.0\n",
      "145\n",
      "(array([-0.47806716, -0.00503641], dtype=float32), -1.0, False, False, {})\n",
      "-146.0\n",
      "146\n",
      "(array([-0.482444  , -0.00437683], dtype=float32), -1.0, False, False, {})\n",
      "-147.0\n",
      "147\n",
      "(array([-0.4861287 , -0.00368471], dtype=float32), -1.0, False, False, {})\n",
      "-148.0\n",
      "148\n",
      "(array([-0.48909384, -0.00296514], dtype=float32), -1.0, False, False, {})\n",
      "-149.0\n",
      "149\n",
      "(array([-0.4913173 , -0.00222347], dtype=float32), -1.0, False, False, {})\n",
      "-150.0\n",
      "150\n",
      "(array([-0.4927825, -0.0014652], dtype=float32), -1.0, False, False, {})\n",
      "-151.0\n",
      "151\n",
      "(array([-0.4934785, -0.000696 ], dtype=float32), -1.0, False, False, {})\n",
      "-152.0\n",
      "152\n",
      "(array([-4.9340010e-01,  7.8409714e-05], dtype=float32), -1.0, False, False, {})\n",
      "-153.0\n",
      "153\n",
      "(array([-0.49254787,  0.00085223], dtype=float32), -1.0, False, False, {})\n",
      "-154.0\n",
      "154\n",
      "(array([-0.49092817,  0.00161968], dtype=float32), -1.0, False, False, {})\n",
      "-155.0\n",
      "155\n",
      "(array([-0.48855314,  0.00237505], dtype=float32), -1.0, False, False, {})\n",
      "-156.0\n",
      "156\n",
      "(array([-0.48544043,  0.00311269], dtype=float32), -1.0, False, False, {})\n",
      "-157.0\n",
      "157\n",
      "(array([-0.4816133 ,  0.00382713], dtype=float32), -1.0, False, False, {})\n",
      "-158.0\n",
      "158\n",
      "(array([-0.47710025,  0.00451307], dtype=float32), -1.0, False, False, {})\n",
      "-159.0\n",
      "159\n",
      "(array([-0.4719348 ,  0.00516546], dtype=float32), -1.0, False, False, {})\n",
      "-160.0\n",
      "160\n",
      "(array([-0.46615526,  0.00577953], dtype=float32), -1.0, False, False, {})\n",
      "-161.0\n",
      "161\n",
      "(array([-0.45980445,  0.00635083], dtype=float32), -1.0, False, False, {})\n",
      "-162.0\n",
      "162\n",
      "(array([-0.45292914,  0.00687529], dtype=float32), -1.0, False, False, {})\n",
      "-163.0\n",
      "163\n",
      "(array([-0.44557992,  0.00734923], dtype=float32), -1.0, False, False, {})\n",
      "-164.0\n",
      "164\n",
      "(array([-0.4378105 ,  0.00776941], dtype=float32), -1.0, False, False, {})\n",
      "-165.0\n",
      "165\n",
      "(array([-0.42967743,  0.00813308], dtype=float32), -1.0, False, False, {})\n",
      "-166.0\n",
      "166\n",
      "(array([-0.42123947,  0.00843795], dtype=float32), -1.0, False, False, {})\n",
      "-167.0\n",
      "167\n",
      "(array([-0.4125572 ,  0.00868227], dtype=float32), -1.0, False, False, {})\n",
      "-168.0\n",
      "168\n",
      "(array([-0.40369242,  0.00886478], dtype=float32), -1.0, False, False, {})\n",
      "-169.0\n",
      "169\n",
      "(array([-0.39470768,  0.00898475], dtype=float32), -1.0, False, False, {})\n",
      "-170.0\n",
      "170\n",
      "(array([-0.3856657 ,  0.00904197], dtype=float32), -1.0, False, False, {})\n",
      "-171.0\n",
      "171\n",
      "(array([-0.37662897,  0.00903675], dtype=float32), -1.0, False, False, {})\n",
      "-172.0\n",
      "172\n",
      "(array([-0.36765912,  0.00896984], dtype=float32), -1.0, False, False, {})\n",
      "-173.0\n",
      "173\n",
      "(array([-0.35881662,  0.00884249], dtype=float32), -1.0, False, False, {})\n",
      "-174.0\n",
      "174\n",
      "(array([-0.35016027,  0.00865635], dtype=float32), -1.0, False, False, {})\n",
      "-175.0\n",
      "175\n",
      "(array([-0.3417468 ,  0.00841346], dtype=float32), -1.0, False, False, {})\n",
      "-176.0\n",
      "176\n",
      "(array([-0.3336306 ,  0.00811623], dtype=float32), -1.0, False, False, {})\n",
      "-177.0\n",
      "177\n",
      "(array([-0.32586324,  0.00776735], dtype=float32), -1.0, False, False, {})\n",
      "-178.0\n",
      "178\n",
      "(array([-0.31849346,  0.00736979], dtype=float32), -1.0, False, False, {})\n",
      "-179.0\n",
      "179\n",
      "(array([-0.31156668,  0.00692675], dtype=float32), -1.0, False, False, {})\n",
      "-180.0\n",
      "180\n",
      "(array([-0.3051251,  0.0064416], dtype=float32), -1.0, False, False, {})\n",
      "-181.0\n",
      "181\n",
      "(array([-0.2992072 ,  0.00591787], dtype=float32), -1.0, False, False, {})\n",
      "-182.0\n",
      "182\n",
      "(array([-0.29384804,  0.00535919], dtype=float32), -1.0, False, False, {})\n",
      "-183.0\n",
      "183\n",
      "(array([-0.28907874,  0.00476929], dtype=float32), -1.0, False, False, {})\n",
      "-184.0\n",
      "184\n",
      "(array([-0.28492677,  0.00415195], dtype=float32), -1.0, False, False, {})\n",
      "-185.0\n",
      "185\n",
      "(array([-0.2814158 ,  0.00351099], dtype=float32), -1.0, False, False, {})\n",
      "-186.0\n",
      "186\n",
      "(array([-0.27856553,  0.00285025], dtype=float32), -1.0, False, False, {})\n",
      "-187.0\n",
      "187\n",
      "(array([-0.27639192,  0.0021736 ], dtype=float32), -1.0, False, False, {})\n",
      "-188.0\n",
      "188\n",
      "(array([-0.27490705,  0.00148489], dtype=float32), -1.0, False, False, {})\n",
      "-189.0\n",
      "189\n",
      "(array([-0.27411905,  0.00078799], dtype=float32), -1.0, False, False, {})\n",
      "-190.0\n",
      "190\n",
      "(array([-2.740323e-01,  8.674851e-05], dtype=float32), -1.0, False, False, {})\n",
      "-191.0\n",
      "191\n",
      "(array([-0.27464727, -0.00061497], dtype=float32), -1.0, False, False, {})\n",
      "-192.0\n",
      "192\n",
      "(array([-0.27596056, -0.0013133 ], dtype=float32), -1.0, False, False, {})\n",
      "-193.0\n",
      "193\n",
      "(array([-0.27796498, -0.0020044 ], dtype=float32), -1.0, False, False, {})\n",
      "-194.0\n",
      "194\n",
      "(array([-0.28064936, -0.00268439], dtype=float32), -1.0, False, False, {})\n",
      "-195.0\n",
      "195\n",
      "(array([-0.2839988 , -0.00334941], dtype=float32), -1.0, False, False, {})\n",
      "-196.0\n",
      "196\n",
      "(array([-0.28799438, -0.00399562], dtype=float32), -1.0, False, False, {})\n",
      "-197.0\n",
      "197\n",
      "(array([-0.29261354, -0.00461915], dtype=float32), -1.0, False, False, {})\n",
      "-198.0\n",
      "198\n",
      "(array([-0.29782975, -0.00521619], dtype=float32), -1.0, False, False, {})\n",
      "-199.0\n",
      "199\n",
      "(array([-0.30361268, -0.00578293], dtype=float32), -1.0, False, True, {})\n",
      "-200.0\n",
      "200\n",
      "(array([-0.3099283 , -0.00631564], dtype=float32), -1.0, False, True, {})\n",
      "-201.0\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(201):\n",
    "    print(i)\n",
    "    a = env.step(2)\n",
    "    sum += a[1]\n",
    "    print(a)\n",
    "    print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-201.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruedaPcEuf4N"
   },
   "source": [
    "### Code Pierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "1toDudAwuUiX"
   },
   "outputs": [],
   "source": [
    "def on_policy_monte_carlo(\n",
    "    env,\n",
    "    nb_states,\n",
    "    nb_actions,\n",
    "    encode_fct,\n",
    "    nb_episodes=20000,\n",
    "    gamma=1,\n",
    "    epsilon=0.1,\n",
    "    use_glei=False,\n",
    "    decay_rate=0.2,\n",
    "    min_epsilon=0.05,\n",
    "):\n",
    "    \"\"\"\n",
    "    This algorithm goal is to find the optimal policy in a episodic environement with reasonable number of states and actions\n",
    "    Convergence guaranteed in this paper: https://sites.ualberta.ca/~szepesva/papers/sarsa98.ps.pdf\n",
    "\n",
    "    Repeat for the given number of episodes:\n",
    "        Simulate an episode with the given policy\n",
    "        Compute the accumulated rewards with discount factor (gamma): G\n",
    "        For every visited q(s, a) in the current episode:\n",
    "            If first visit of (s, a) is at iteration t:\n",
    "                acc(s, a) += Gt\n",
    "                count(s, a) += 1\n",
    "                q(s, a) = acc(s, a) / count(s, a)\n",
    "            For every state in the current episode:\n",
    "                update policy(s, a) with an epsilon greedy strategy or GLEI strategy\n",
    "\n",
    "    Args:\n",
    "        - env (gymnasium.Env): a gymnasium environment\n",
    "        - nb_states (int): the number of states for the given env\n",
    "        - nb_actions (int): the number of actions for the given env\n",
    "        - encode_fct (callable): the function to encode our state.\n",
    "        - nb_episodes (int): number of episodes simulated for learning\n",
    "        - gamma (float): discount factor for updating states values\n",
    "        - epsilon (float): initial exploration factor, in [0, 1], if use_gley = True, represent the starting epsilon\n",
    "        - use_glei (bool): if True, use GLEI policy (decaying epsilon across episodes); if False, use simple epsilon-greedy\n",
    "        - decay_rate (float): Every time num_episode > nb_episode * (decay_rate *i) then epsilon = epsilon * (1 - decay_rate) * i. Where i is the number of time we decayed epsilon\n",
    "        - min_epsilon (float): the minimum decayed epsilon when using a GLEI policy\n",
    "\n",
    "    Returns:\n",
    "        - policy (numpy.array): the learnt policy\n",
    "        - q (numpy.array): the Q-values\n",
    "        - rewards_historic (list): the historic of rewards during training\n",
    "    \"\"\"\n",
    "    # Initialize algorithm variables\n",
    "    q = np.zeros(\n",
    "        shape=(nb_states, nb_actions)\n",
    "    )  # (state, action) values through every episodes generated\n",
    "    count = np.zeros(\n",
    "        shape=(nb_states, nb_actions)\n",
    "    )  # Counting each (s, a) events through every episodes generated\n",
    "    acc = np.zeros(\n",
    "        shape=(nb_states, nb_actions)\n",
    "    )  # (state, action) cumulated values through every episodes generated\n",
    "    policy = np.full(\n",
    "        shape=(nb_states, nb_actions), fill_value=1 / nb_actions\n",
    "    )  # Actions probabilities for each state. Initialized as a uniform policy\n",
    "    rewards_historic = []\n",
    "    count_decay = 1  # Decay rate factor\n",
    "\n",
    "    for num_episode in range(nb_episodes):\n",
    "        # print(f\"Computing ep {num_episode}\")\n",
    "\n",
    "        # Initialize environment\n",
    "        game_state = env.reset()[\n",
    "            0\n",
    "        ]  # A tuple (player score, opponent score, ace (1) or not (0))\n",
    "        terminated = False\n",
    "        G = np.empty((0, 1))  # Cumulated rewards of the current episode\n",
    "        moves_historic = np.empty(\n",
    "            (0, 2), dtype=int\n",
    "        )  # Historic of (state, action) of the current episode\n",
    "\n",
    "        # Step 1: Simulate an episode\n",
    "        while not terminated:\n",
    "            # Draw an action according to the current policy\n",
    "            state_index = encode_fct(game_state)\n",
    "            action = draw(policy, state_index)\n",
    "\n",
    "            # Update environment with the chosen action\n",
    "            env_state = env.step(action)\n",
    "            game_state, G, terminated = (\n",
    "                env_state[0],\n",
    "                np.append(G, env_state[1]),\n",
    "                env_state[2],\n",
    "            )\n",
    "\n",
    "            # Update current historic if current (state, action) was not visited yet\n",
    "            moves_historic = np.append(moves_historic, [[state_index, action]], axis=0)\n",
    "\n",
    "        # Update rewards_historic with the total reward of the episode\n",
    "        rewards_historic.append(np.sum(G))\n",
    "\n",
    "        # Step 2: Compute G, the cumulative rewards for each state\n",
    "        gamma_powers = gamma ** np.arange(len(G))\n",
    "        for i in range(len(G) - 1):\n",
    "            G[i] = np.sum(G[i:] * gamma_powers[: len(G[i:])])\n",
    "\n",
    "        # Step 3: For every (s, a) visited: Update q(s, a)\n",
    "        # Extracting first visit for every (s, a)\n",
    "        first_visit_states_action, first_visit_indices = np.unique(\n",
    "            moves_historic, axis=0, return_index=True\n",
    "        )\n",
    "        first_visit_indices = np.sort(first_visit_indices)\n",
    "\n",
    "        # Extract relevant states, actions, and rewards for first visits\n",
    "        states = moves_historic[first_visit_indices, 0]\n",
    "        actions = moves_historic[first_visit_indices, 1]\n",
    "        rewards = G[first_visit_indices]\n",
    "\n",
    "        # Update acc, count, and q arrays\n",
    "        np.add.at(acc, (states, actions), rewards)  # Accumulate rewards\n",
    "        np.add.at(count, (states, actions), 1)  # Increment count\n",
    "        q = np.divide(acc, count, where=(count != 0), out=np.zeros_like(acc))\n",
    "\n",
    "        # Step 4: Update policy with the current Q-values\n",
    "        # Update epsilon if we use GLEI policy\n",
    "        if (use_glei) & (num_episode > (decay_rate * count_decay) * nb_episodes):\n",
    "            epsilon = max(epsilon * (1 - (decay_rate * count_decay)), min_epsilon)\n",
    "            count_decay += 1\n",
    "        policy = update_eps_greedy_policy(epsilon=epsilon, q=q)\n",
    "\n",
    "    if use_glei:\n",
    "        print(f\"Final epsilon: {epsilon}\")\n",
    "    return policy, q, rewards_historic"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mountain-car-dUPDKyLO-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
